{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13665369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c269b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=512,\n",
    "            num_classes=18,\n",
    "            activation='relu',\n",
    "            hidden_sizes=[1024, 2048, 1024, 256, 128],\n",
    "            dropout=0.1\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pick activation\n",
    "        if activation == \"relu\":\n",
    "            activation_cls = nn.ReLU\n",
    "        elif activation == \"leaky_relu\":\n",
    "            activation_cls = nn.LeakyReLU\n",
    "        elif activation == \"gelu\":\n",
    "            activation_cls = nn.GELU\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(nn.BatchNorm1d(h))  # helps stabilize\n",
    "            layers.append(activation_cls())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = h\n",
    "\n",
    "        # Final classification layer\n",
    "        layers.append(nn.Linear(in_dim, num_classes))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7aae78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = '/home/free4ky/projects/chest-diseases/model_multilabel20_pr034_rec0777.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133c2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_concat_dataset(embed_paths, label_paths):\n",
    "    \"\"\"\n",
    "    Load multiple .pt files and concatenate along dim 0\n",
    "    \"\"\"\n",
    "    X_list = [torch.load(p) for p in embed_paths]\n",
    "    y_list = [torch.load(p) for p in label_paths]\n",
    "    \n",
    "    X = torch.cat(X_list, dim=0)\n",
    "    y = torch.cat(y_list, dim=0)\n",
    "    \n",
    "    return TensorDataset(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3125f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Computing probabilities on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3133 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3133/3133 [00:00<00:00, 4584.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Best threshold=0.7061, F1=0.3781\n",
      "Class 1: Best threshold=0.5387, F1=0.6974\n",
      "Class 2: Best threshold=0.8773, F1=0.5885\n",
      "Class 3: Best threshold=0.8802, F1=0.3786\n",
      "Class 4: Best threshold=0.6020, F1=0.6442\n",
      "Class 5: Best threshold=0.6131, F1=0.3719\n",
      "Class 6: Best threshold=0.5477, F1=0.5210\n",
      "Class 7: Best threshold=0.4760, F1=0.4824\n",
      "Class 8: Best threshold=0.5168, F1=0.4488\n",
      "Class 9: Best threshold=0.3881, F1=0.6346\n",
      "Class 10: Best threshold=0.4427, F1=0.6290\n",
      "Class 11: Best threshold=0.4749, F1=0.4716\n",
      "Class 12: Best threshold=0.6319, F1=0.6395\n",
      "Class 13: Best threshold=0.8423, F1=0.3762\n",
      "Class 14: Best threshold=0.7366, F1=0.3939\n",
      "Class 15: Best threshold=0.6886, F1=0.5043\n",
      "Class 16: Best threshold=0.6208, F1=0.3194\n",
      "Class 17: Best threshold=0.7119, F1=0.3768\n",
      "Class 18: Best threshold=0.9429, F1=0.6076\n",
      "Class 19: Best threshold=0.9712, F1=0.6111\n",
      "Optimal thresholds per class: [0.70609677 0.53872406 0.87727875 0.88017046 0.60204166 0.613144\n",
      " 0.5476915  0.47602674 0.5168072  0.38809362 0.44271022 0.47488382\n",
      " 0.6318726  0.8423309  0.7365511  0.6886128  0.6208342  0.711936\n",
      " 0.94288176 0.9711801 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load model\n",
    "# -------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# input_size: 512\n",
    "# batch_size: 2048\n",
    "# activation: leaky_relu\n",
    "# dropout: 0.2\n",
    "# hidden_sizes:\n",
    "# - 512\n",
    "# - 256\n",
    "# - 128\n",
    "model = MLP(\n",
    "    input_size=512,\n",
    "    activation='leaky_relu',\n",
    "    dropout=0.2,\n",
    "    num_classes=20,  # e.g., 20\n",
    "    hidden_sizes=[512,256,128]\n",
    ")\n",
    "\n",
    "state_dict = torch.load(MODEL_CKPT, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device).eval()\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Load dataset\n",
    "# -------------------------\n",
    "val_ds = _load_concat_dataset(\n",
    "['/home/free4ky/projects/chest-diseases/data/preprocessed_mosmed/test_data.pt',\n",
    " '/home/free4ky/projects/chest-diseases/data/preprocessed_val_20/validation_data.pt'\n",
    "],\n",
    "[\n",
    "'/home/free4ky/projects/chest-diseases/data/preprocessed_mosmed/test_labels.pt',\n",
    "'/home/free4ky/projects/chest-diseases/data/preprocessed_val_20/validation_labels.pt'\n",
    "]\n",
    ")\n",
    "val_dl = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Inference\n",
    "# -------------------------\n",
    "print(\"Computing probabilities on validation set...\")\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for emb, y in tqdm(val_dl):\n",
    "        emb = torch.nn.functional.normalize(emb, dim=-1)\n",
    "        logits = model(emb.to(device))\n",
    "        probs = torch.sigmoid(logits)  # multilabel, sigmoid per class\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_labels.append(y)\n",
    "\n",
    "all_probs = torch.cat(all_probs, dim=0).numpy()  # shape [num_samples, num_classes]\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "# -------------------------\n",
    "# 4. Compute best threshold per class\n",
    "# -------------------------\n",
    "best_thresholds = []\n",
    "\n",
    "for i in range(all_labels.shape[1]):\n",
    "    y_true = all_labels[:, i]\n",
    "    y_prob = all_probs[:, i]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1_scores = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    best_thresholds.append(best_threshold)\n",
    "    print(f\"Class {i}: Best threshold={best_threshold:.4f}, F1={f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "best_thresholds = np.array(best_thresholds)\n",
    "print(f\"Optimal thresholds per class: {best_thresholds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11d5164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70609677, 0.53872406, 0.87727875, 0.88017046, 0.60204166,\n",
       "       0.613144  , 0.5476915 , 0.47602674, 0.5168072 , 0.38809362,\n",
       "       0.44271022, 0.47488382, 0.6318726 , 0.8423309 , 0.7365511 ,\n",
       "       0.6886128 , 0.6208342 , 0.711936  , 0.94288176, 0.9711801 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e33bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.5008, Micro F1: 0.5252\n",
      "Precision (macro): 0.4087, Recall (macro): 0.6692\n",
      "Accuracy: 0.0753\n",
      "AUROC macro: 0.8040248288977032\n",
      "AU-PR macro: 0.38417851807324566\n",
      "Per-class F1: [0.37653737 0.69681104 0.58679707 0.37614679 0.64343164 0.37055418\n",
      " 0.5202934  0.48150256 0.44804866 0.63427242 0.62849258 0.47098976\n",
      " 0.63796909 0.37396122 0.39240506 0.50326323 0.31789282 0.37485172\n",
      " 0.58974359 0.5915493 ]\n",
      "Per-class AUROC: [0.75454648 0.87355278 0.91557199 0.8004268  0.86182764 0.74423348\n",
      " 0.72725877 0.77809975 0.6984549  0.67501074 0.73360559 0.6638663\n",
      " 0.92844327 0.82221056 0.77672761 0.78473298 0.73475929 0.82065572\n",
      " 0.99334527 0.99316668]\n",
      "Per-class AU-PR: [0.25831469 0.67865456 0.53710951 0.23486803 0.61553397 0.28869146\n",
      " 0.45929579 0.47251787 0.39937928 0.60204746 0.61789258 0.39263981\n",
      " 0.6584722  0.27616092 0.28926137 0.45531531 0.22114663 0.26570183\n",
      " 0.58131068 0.61506464]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "# -------------------------\n",
    "# 5. Binarize predictions using best thresholds\n",
    "# -------------------------\n",
    "binary_preds = (all_probs > best_thresholds).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Compute metrics\n",
    "# -------------------------\n",
    "# Macro F1\n",
    "f1_macro = f1_score(all_labels, binary_preds, average=\"macro\")\n",
    "# Micro F1\n",
    "f1_micro = f1_score(all_labels, binary_preds, average=\"micro\")\n",
    "# Per-class F1\n",
    "f1_per_class = f1_score(all_labels, binary_preds, average=None)\n",
    "\n",
    "# Precision & Recall (macro)\n",
    "precision_macro = precision_score(all_labels, binary_preds, average=\"macro\")\n",
    "recall_macro = recall_score(all_labels, binary_preds, average=\"macro\")\n",
    "\n",
    "# Accuracy (sample-level)\n",
    "accuracy = accuracy_score(all_labels, binary_preds)\n",
    "\n",
    "# AUROC\n",
    "try:\n",
    "    auroc_macro = roc_auc_score(all_labels, all_probs, average=\"macro\")\n",
    "    auroc_per_class = roc_auc_score(all_labels, all_probs, average=None)\n",
    "except ValueError:\n",
    "    auroc_macro = None\n",
    "    auroc_per_class = None\n",
    "\n",
    "# -------------------------\n",
    "# AU-PR per class and macro\n",
    "# -------------------------\n",
    "aupr_per_class = []\n",
    "\n",
    "for i in range(all_labels.shape[1]):\n",
    "    y_true = all_labels[:, i]\n",
    "    y_prob = all_probs[:, i]\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    # recall decreases, so reverse both arrays to make x-axis increasing\n",
    "    aupr_per_class.append(auc(recall[::-1], precision[::-1]))\n",
    "\n",
    "aupr_per_class = np.array(aupr_per_class)\n",
    "\n",
    "# Macro AU-PR: flatten all classes\n",
    "precision_flat, recall_flat, _ = precision_recall_curve(\n",
    "    all_labels.flatten(), all_probs.flatten()\n",
    ")\n",
    "aupr_macro = auc(recall_flat[::-1], precision_flat[::-1])\n",
    "\n",
    "# -------------------------\n",
    "# 7. Print metrics\n",
    "# -------------------------\n",
    "print(f\"Macro F1: {f1_macro:.4f}, Micro F1: {f1_micro:.4f}\")\n",
    "print(f\"Precision (macro): {precision_macro:.4f}, Recall (macro): {recall_macro:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUROC macro: {auroc_macro}\")\n",
    "print(f\"AU-PR macro: {aupr_macro}\")\n",
    "print(f\"Per-class F1: {f1_per_class}\")\n",
    "print(f\"Per-class AUROC: {auroc_per_class}\")\n",
    "print(f\"Per-class AU-PR: {aupr_per_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bb64f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 1, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]], shape=(3133, 20))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60309f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21652523, 0.8563636 , 0.02323474, ..., 0.02188679, 0.08152154,\n",
       "        0.13146359],\n",
       "       [0.01217982, 0.01380807, 0.02129888, ..., 0.01234266, 0.9964037 ,\n",
       "        0.51302296],\n",
       "       [0.01879999, 0.01722941, 0.0245573 , ..., 0.01340362, 0.98717   ,\n",
       "        0.98037535],\n",
       "       ...,\n",
       "       [0.01326523, 0.0242513 , 0.00580554, ..., 0.00947404, 0.03806407,\n",
       "        0.0230825 ],\n",
       "       [0.1676907 , 0.54416275, 0.8802362 , ..., 0.68881893, 0.03004128,\n",
       "        0.01985495],\n",
       "       [0.0692402 , 0.46907988, 0.49736205, ..., 0.5212696 , 0.02730496,\n",
       "        0.00875787]], shape=(3133, 20), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b9a3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def compute_youden_thresholds(y_true: np.ndarray, y_probs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute per-class thresholds using Youden's J statistic.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): shape [num_samples, num_classes], binary labels\n",
    "        y_probs (np.ndarray): shape [num_samples, num_classes], predicted probabilities\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: per-class thresholds\n",
    "    \"\"\"\n",
    "    num_classes = y_true.shape[1]\n",
    "    thresholds = np.zeros(num_classes, dtype=float)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        y_c = y_true[:, i]\n",
    "        p_c = y_probs[:, i]\n",
    "\n",
    "        if np.unique(y_c).size < 2:\n",
    "            # Class not present in validation, fallback\n",
    "            thresholds[i] = 0.5\n",
    "            continue\n",
    "\n",
    "        fpr, tpr, thr = roc_curve(y_c, p_c)\n",
    "        j_scores = tpr - fpr\n",
    "        best_idx = np.argmax(j_scores)\n",
    "        thresholds[i] = thr[best_idx]\n",
    "\n",
    "    return thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ad74918",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds = compute_youden_thresholds(all_labels, all_probs)\n",
    "binary_preds = (all_probs > best_thresholds).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54706a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.4560, Micro F1: 0.4612\n",
      "Precision (macro): 0.3473, Recall (macro): 0.7485\n",
      "Accuracy: 0.0361\n",
      "AUROC macro: 0.7806986463679311\n",
      "AU-PR macro: 0.41625180442996307\n",
      "Per-class F1: [0.35174954 0.6819222  0.51056015 0.32933479 0.63254862 0.352473\n",
      " 0.49455984 0.45404814 0.42950326 0.59057239 0.57673267 0.44187009\n",
      " 0.59513075 0.29705882 0.29417773 0.48764629 0.24307837 0.29002154\n",
      " 0.60504202 0.46153846]\n",
      "Per-class AUROC: [0.75064464 0.86808149 0.90605194 0.77552201 0.84996053 0.73821267\n",
      " 0.69849185 0.73867219 0.66306608 0.65426846 0.70846969 0.6184885\n",
      " 0.92538625 0.7885101  0.72387978 0.7529817  0.65438221 0.81139259\n",
      " 0.99248338 0.99502686]\n",
      "Per-class AU-PR: [0.26142292 0.66527927 0.49897836 0.28077083 0.57010477 0.2916531\n",
      " 0.4108997  0.42890124 0.32771492 0.57235455 0.57916542 0.35877698\n",
      " 0.68927743 0.267614   0.235799   0.39422172 0.17461135 0.2542383\n",
      " 0.68605984 0.69032716]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "# -------------------------\n",
    "# 5. Binarize predictions using best thresholds\n",
    "# -------------------------\n",
    "binary_preds = (all_probs > best_thresholds).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Compute metrics\n",
    "# -------------------------\n",
    "# Macro F1\n",
    "f1_macro = f1_score(all_labels, binary_preds, average=\"macro\")\n",
    "# Micro F1\n",
    "f1_micro = f1_score(all_labels, binary_preds, average=\"micro\")\n",
    "# Per-class F1\n",
    "f1_per_class = f1_score(all_labels, binary_preds, average=None)\n",
    "\n",
    "# Precision & Recall (macro)\n",
    "precision_macro = precision_score(all_labels, binary_preds, average=\"macro\")\n",
    "recall_macro = recall_score(all_labels, binary_preds, average=\"macro\")\n",
    "\n",
    "# Accuracy (sample-level)\n",
    "accuracy = accuracy_score(all_labels, binary_preds)\n",
    "\n",
    "# AUROC\n",
    "try:\n",
    "    auroc_macro = roc_auc_score(all_labels, all_probs, average=\"macro\")\n",
    "    auroc_per_class = roc_auc_score(all_labels, all_probs, average=None)\n",
    "except ValueError:\n",
    "    auroc_macro = None\n",
    "    auroc_per_class = None\n",
    "\n",
    "# -------------------------\n",
    "# AU-PR per class and macro\n",
    "# -------------------------\n",
    "aupr_per_class = []\n",
    "\n",
    "for i in range(all_labels.shape[1]):\n",
    "    y_true = all_labels[:, i]\n",
    "    y_prob = all_probs[:, i]\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    # recall decreases, so reverse both arrays to make x-axis increasing\n",
    "    aupr_per_class.append(auc(recall[::-1], precision[::-1]))\n",
    "\n",
    "aupr_per_class = np.array(aupr_per_class)\n",
    "\n",
    "# Macro AU-PR: flatten all classes\n",
    "precision_flat, recall_flat, _ = precision_recall_curve(\n",
    "    all_labels.flatten(), all_probs.flatten()\n",
    ")\n",
    "aupr_macro = auc(recall_flat[::-1], precision_flat[::-1])\n",
    "\n",
    "# -------------------------\n",
    "# 7. Print metrics\n",
    "# -------------------------\n",
    "print(f\"Macro F1: {f1_macro:.4f}, Micro F1: {f1_micro:.4f}\")\n",
    "print(f\"Precision (macro): {precision_macro:.4f}, Recall (macro): {recall_macro:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUROC macro: {auroc_macro}\")\n",
    "print(f\"AU-PR macro: {aupr_macro}\")\n",
    "print(f\"Per-class F1: {f1_per_class}\")\n",
    "print(f\"Per-class AUROC: {auroc_per_class}\")\n",
    "print(f\"Per-class AU-PR: {aupr_per_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dafa8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
