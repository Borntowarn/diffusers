{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ea449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf7b281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено файлов в embeds: 319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Совпадают (allclose): 319\n",
      "Не совпадают: 0\n",
      "Нет в orig: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Пути к папкам\n",
    "embeds_root = \"/home/borntowarn/projects/chest-diseases/training/data/CT-RATE/dataset/valid_fixed_embeds\"\n",
    "orig_root = \"/home/borntowarn/projects/chest-diseases/training/data/CT-RATE/cached_latents/vit_0_1\"\n",
    "\n",
    "# Собираем все пути к .pt файлам в embeds (с учетом иерархии)\n",
    "embeds_pt_files = []\n",
    "for root, dirs, files in os.walk(embeds_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pt\"):\n",
    "            embeds_pt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Найдено файлов в embeds: {len(embeds_pt_files)}\")\n",
    "\n",
    "not_allclose = []\n",
    "allclose = []\n",
    "not_found_in_orig = []\n",
    "\n",
    "for embed_path in embeds_pt_files:\n",
    "    # Имя файла без пути\n",
    "    fname = os.path.basename(embed_path)\n",
    "    # В orig названия файлов без расширения .nii\n",
    "    # Например: если embed_path = .../xxx.nii.pt, то в orig это xxx.pt\n",
    "    if fname.endswith(\".nii.pt\"):\n",
    "        orig_fname = fname.replace(\".nii.pt\", \".pt\")\n",
    "    else:\n",
    "        orig_fname = fname\n",
    "    orig_path = os.path.join(orig_root, orig_fname)\n",
    "    if not os.path.exists(orig_path):\n",
    "        not_found_in_orig.append(fname)\n",
    "        continue\n",
    "    try:\n",
    "        t1 = torch.load(embed_path, map_location=\"cpu\")\n",
    "        import torch.nn.functional as F\n",
    "        t1 = F.normalize(t1, dim=-1)\n",
    "        t2 = torch.load(orig_path, map_location=\"cpu\")\n",
    "        if torch.allclose(t1.cpu(), t2.cpu(), atol=1e-5, rtol=1e-3):\n",
    "            allclose.append(fname)\n",
    "        else:\n",
    "            not_allclose.append(fname)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сравнении {fname}: {e}\")\n",
    "        not_allclose.append(fname)\n",
    "\n",
    "print(f\"Совпадают (allclose): {len(allclose)}\")\n",
    "print(f\"Не совпадают: {len(not_allclose)}\")\n",
    "print(f\"Нет в orig: {len(not_found_in_orig)}\")\n",
    "\n",
    "if not_allclose:\n",
    "    print(\"Не совпадающие файлы:\")\n",
    "    for f in not_allclose:\n",
    "        print(f)\n",
    "if not_found_in_orig:\n",
    "    print(\"Файлы, которых нет в orig:\")\n",
    "    for f in not_found_in_orig:\n",
    "        print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ffd4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('/home/borntowarn/projects/chest-diseases/training/weights/CT-CLIP/CT_LiPro_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e2efee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['trained_model.temperature', 'trained_model.text_transformer.embeddings.position_ids', 'trained_model.text_transformer.embeddings.word_embeddings.weight', 'trained_model.text_transformer.embeddings.position_embeddings.weight', 'trained_model.text_transformer.embeddings.token_type_embeddings.weight', 'trained_model.text_transformer.embeddings.LayerNorm.weight', 'trained_model.text_transformer.embeddings.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.0.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.0.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.0.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.0.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.0.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.0.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.0.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.0.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.0.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.0.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.0.output.dense.weight', 'trained_model.text_transformer.encoder.layer.0.output.dense.bias', 'trained_model.text_transformer.encoder.layer.0.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.0.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.1.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.1.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.1.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.1.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.1.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.1.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.1.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.1.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.1.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.1.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.1.output.dense.weight', 'trained_model.text_transformer.encoder.layer.1.output.dense.bias', 'trained_model.text_transformer.encoder.layer.1.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.1.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.2.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.2.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.2.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.2.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.2.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.2.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.2.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.2.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.2.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.2.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.2.output.dense.weight', 'trained_model.text_transformer.encoder.layer.2.output.dense.bias', 'trained_model.text_transformer.encoder.layer.2.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.2.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.3.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.3.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.3.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.3.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.3.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.3.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.3.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.3.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.3.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.3.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.3.output.dense.weight', 'trained_model.text_transformer.encoder.layer.3.output.dense.bias', 'trained_model.text_transformer.encoder.layer.3.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.3.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.4.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.4.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.4.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.4.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.4.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.4.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.4.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.4.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.4.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.4.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.4.output.dense.weight', 'trained_model.text_transformer.encoder.layer.4.output.dense.bias', 'trained_model.text_transformer.encoder.layer.4.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.4.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.5.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.5.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.5.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.5.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.5.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.5.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.5.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.5.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.5.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.5.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.5.output.dense.weight', 'trained_model.text_transformer.encoder.layer.5.output.dense.bias', 'trained_model.text_transformer.encoder.layer.5.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.5.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.6.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.6.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.6.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.6.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.6.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.6.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.6.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.6.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.6.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.6.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.6.output.dense.weight', 'trained_model.text_transformer.encoder.layer.6.output.dense.bias', 'trained_model.text_transformer.encoder.layer.6.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.6.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.7.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.7.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.7.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.7.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.7.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.7.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.7.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.7.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.7.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.7.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.7.output.dense.weight', 'trained_model.text_transformer.encoder.layer.7.output.dense.bias', 'trained_model.text_transformer.encoder.layer.7.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.7.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.8.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.8.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.8.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.8.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.8.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.8.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.8.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.8.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.8.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.8.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.8.output.dense.weight', 'trained_model.text_transformer.encoder.layer.8.output.dense.bias', 'trained_model.text_transformer.encoder.layer.8.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.8.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.9.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.9.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.9.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.9.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.9.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.9.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.9.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.9.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.9.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.9.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.9.output.dense.weight', 'trained_model.text_transformer.encoder.layer.9.output.dense.bias', 'trained_model.text_transformer.encoder.layer.9.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.9.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.10.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.10.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.10.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.10.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.10.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.10.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.10.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.10.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.10.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.10.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.10.output.dense.weight', 'trained_model.text_transformer.encoder.layer.10.output.dense.bias', 'trained_model.text_transformer.encoder.layer.10.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.10.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.11.attention.self.query.weight', 'trained_model.text_transformer.encoder.layer.11.attention.self.query.bias', 'trained_model.text_transformer.encoder.layer.11.attention.self.key.weight', 'trained_model.text_transformer.encoder.layer.11.attention.self.key.bias', 'trained_model.text_transformer.encoder.layer.11.attention.self.value.weight', 'trained_model.text_transformer.encoder.layer.11.attention.self.value.bias', 'trained_model.text_transformer.encoder.layer.11.attention.output.dense.weight', 'trained_model.text_transformer.encoder.layer.11.attention.output.dense.bias', 'trained_model.text_transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'trained_model.text_transformer.encoder.layer.11.intermediate.dense.weight', 'trained_model.text_transformer.encoder.layer.11.intermediate.dense.bias', 'trained_model.text_transformer.encoder.layer.11.output.dense.weight', 'trained_model.text_transformer.encoder.layer.11.output.dense.bias', 'trained_model.text_transformer.encoder.layer.11.output.LayerNorm.weight', 'trained_model.text_transformer.encoder.layer.11.output.LayerNorm.bias', 'trained_model.text_transformer.pooler.dense.weight', 'trained_model.text_transformer.pooler.dense.bias', 'trained_model.visual_transformer.spatial_rel_pos_bias.net.0.0.weight', 'trained_model.visual_transformer.spatial_rel_pos_bias.net.0.0.bias', 'trained_model.visual_transformer.spatial_rel_pos_bias.net.1.0.weight', 'trained_model.visual_transformer.spatial_rel_pos_bias.net.1.0.bias', 'trained_model.visual_transformer.spatial_rel_pos_bias.net.2.weight', 'trained_model.visual_transformer.spatial_rel_pos_bias.net.2.bias', 'trained_model.visual_transformer.to_patch_emb_first_frame.1.weight', 'trained_model.visual_transformer.to_patch_emb_first_frame.1.bias', 'trained_model.visual_transformer.to_patch_emb_first_frame.2.weight', 'trained_model.visual_transformer.to_patch_emb_first_frame.2.bias', 'trained_model.visual_transformer.to_patch_emb_first_frame.3.weight', 'trained_model.visual_transformer.to_patch_emb_first_frame.3.bias', 'trained_model.visual_transformer.to_patch_emb.1.weight', 'trained_model.visual_transformer.to_patch_emb.1.bias', 'trained_model.visual_transformer.to_patch_emb.2.weight', 'trained_model.visual_transformer.to_patch_emb.2.bias', 'trained_model.visual_transformer.to_patch_emb.3.weight', 'trained_model.visual_transformer.to_patch_emb.3.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.0.dsconv.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.0.dsconv.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.null_kv', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.q_scale', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.k_scale', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.norm.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.norm.beta', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.context_norm.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.context_norm.beta', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.to_kv.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.to_out.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.3.0.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.3.0.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.3.1.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.3.4.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.0.dsconv.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.0.dsconv.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.null_kv', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.q_scale', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.k_scale', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.norm.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.norm.beta', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.context_norm.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.context_norm.beta', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.to_q.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.to_kv.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.to_out.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.3.0.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.3.0.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.3.1.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.3.4.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.0.dsconv.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.0.dsconv.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.null_kv', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.q_scale', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.k_scale', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.norm.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.norm.beta', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.context_norm.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.context_norm.beta', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.to_q.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.to_kv.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.to_out.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.3.0.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.3.0.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.3.1.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.3.4.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.0.dsconv.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.0.dsconv.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.null_kv', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.q_scale', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.k_scale', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.norm.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.norm.beta', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.context_norm.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.context_norm.beta', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.to_q.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.to_kv.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.to_out.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.3.0.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.3.0.bias', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.3.1.weight', 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.3.4.weight', 'trained_model.visual_transformer.enc_spatial_transformer.norm_out.gamma', 'trained_model.visual_transformer.enc_spatial_transformer.norm_out.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.0.dsconv.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.0.dsconv.bias', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.null_kv', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.q_scale', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.k_scale', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.norm.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.norm.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.context_norm.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.context_norm.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.to_q.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.to_kv.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.to_out.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.3.0.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.3.0.bias', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.3.1.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.3.4.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.0.dsconv.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.0.dsconv.bias', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.null_kv', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.q_scale', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.k_scale', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.norm.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.norm.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.context_norm.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.context_norm.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.to_q.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.to_kv.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.to_out.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.3.0.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.3.0.bias', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.3.1.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.3.4.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.0.dsconv.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.0.dsconv.bias', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.null_kv', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.q_scale', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.k_scale', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.norm.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.norm.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.context_norm.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.context_norm.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.to_q.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.to_kv.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.to_out.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.3.0.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.3.0.bias', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.3.1.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.3.4.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.0.dsconv.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.0.dsconv.bias', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.null_kv', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.q_scale', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.k_scale', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.norm.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.norm.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.context_norm.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.context_norm.beta', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.to_q.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.to_kv.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.to_out.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.3.0.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.3.0.bias', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.3.1.weight', 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.3.4.weight', 'trained_model.visual_transformer.enc_temporal_transformer.norm_out.gamma', 'trained_model.visual_transformer.enc_temporal_transformer.norm_out.beta', 'trained_model.visual_transformer.vq._codebook.initted', 'trained_model.visual_transformer.vq._codebook.cluster_size', 'trained_model.visual_transformer.vq._codebook.embed', 'trained_model.visual_transformer.to_pixels_first_frame.0.weight', 'trained_model.visual_transformer.to_pixels_first_frame.0.bias', 'trained_model.visual_transformer.to_pixels.0.weight', 'trained_model.visual_transformer.to_pixels.0.bias', 'trained_model.to_text_latent.weight', 'trained_model.to_visual_latent.weight', 'trained_model.to_text_latent_extra.weight', 'trained_model.to_visual_latent_extra.weight', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fd2374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_model.visual_transformer.spatial_rel_pos_bias.net.0.0.weight',\n",
       " 'trained_model.visual_transformer.spatial_rel_pos_bias.net.0.0.bias',\n",
       " 'trained_model.visual_transformer.spatial_rel_pos_bias.net.1.0.weight',\n",
       " 'trained_model.visual_transformer.spatial_rel_pos_bias.net.1.0.bias',\n",
       " 'trained_model.visual_transformer.spatial_rel_pos_bias.net.2.weight',\n",
       " 'trained_model.visual_transformer.spatial_rel_pos_bias.net.2.bias',\n",
       " 'trained_model.visual_transformer.to_patch_emb_first_frame.1.weight',\n",
       " 'trained_model.visual_transformer.to_patch_emb_first_frame.1.bias',\n",
       " 'trained_model.visual_transformer.to_patch_emb_first_frame.2.weight',\n",
       " 'trained_model.visual_transformer.to_patch_emb_first_frame.2.bias',\n",
       " 'trained_model.visual_transformer.to_patch_emb_first_frame.3.weight',\n",
       " 'trained_model.visual_transformer.to_patch_emb_first_frame.3.bias',\n",
       " 'trained_model.visual_transformer.to_patch_emb.1.weight',\n",
       " 'trained_model.visual_transformer.to_patch_emb.1.bias',\n",
       " 'trained_model.visual_transformer.to_patch_emb.2.weight',\n",
       " 'trained_model.visual_transformer.to_patch_emb.2.bias',\n",
       " 'trained_model.visual_transformer.to_patch_emb.3.weight',\n",
       " 'trained_model.visual_transformer.to_patch_emb.3.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.0.dsconv.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.0.dsconv.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.null_kv',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.q_scale',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.k_scale',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.norm.beta',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.context_norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.context_norm.beta',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.to_kv.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.1.to_out.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.3.0.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.3.0.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.3.1.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.0.3.4.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.0.dsconv.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.0.dsconv.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.null_kv',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.q_scale',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.k_scale',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.norm.beta',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.context_norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.context_norm.beta',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.to_q.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.to_kv.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.1.to_out.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.3.0.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.3.0.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.3.1.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.1.3.4.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.0.dsconv.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.0.dsconv.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.null_kv',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.q_scale',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.k_scale',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.norm.beta',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.context_norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.context_norm.beta',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.to_q.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.to_kv.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.1.to_out.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.3.0.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.3.0.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.3.1.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.2.3.4.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.0.dsconv.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.0.dsconv.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.null_kv',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.q_scale',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.k_scale',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.norm.beta',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.context_norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.context_norm.beta',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.to_q.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.to_kv.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.1.to_out.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.3.0.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.3.0.bias',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.3.1.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.layers.3.3.4.weight',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.norm_out.gamma',\n",
       " 'trained_model.visual_transformer.enc_spatial_transformer.norm_out.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.0.dsconv.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.0.dsconv.bias',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.null_kv',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.q_scale',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.k_scale',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.norm.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.context_norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.context_norm.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.to_q.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.to_kv.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.1.to_out.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.3.0.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.3.0.bias',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.3.1.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.0.3.4.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.0.dsconv.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.0.dsconv.bias',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.null_kv',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.q_scale',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.k_scale',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.norm.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.context_norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.context_norm.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.to_q.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.to_kv.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.1.to_out.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.3.0.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.3.0.bias',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.3.1.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.1.3.4.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.0.dsconv.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.0.dsconv.bias',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.null_kv',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.q_scale',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.k_scale',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.norm.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.context_norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.context_norm.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.to_q.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.to_kv.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.1.to_out.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.3.0.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.3.0.bias',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.3.1.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.2.3.4.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.0.dsconv.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.0.dsconv.bias',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.null_kv',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.q_scale',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.k_scale',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.norm.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.context_norm.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.context_norm.beta',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.to_q.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.to_kv.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.1.to_out.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.3.0.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.3.0.bias',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.3.1.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.layers.3.3.4.weight',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.norm_out.gamma',\n",
       " 'trained_model.visual_transformer.enc_temporal_transformer.norm_out.beta',\n",
       " 'trained_model.visual_transformer.vq._codebook.initted',\n",
       " 'trained_model.visual_transformer.vq._codebook.cluster_size',\n",
       " 'trained_model.visual_transformer.vq._codebook.embed',\n",
       " 'trained_model.visual_transformer.to_pixels_first_frame.0.weight',\n",
       " 'trained_model.visual_transformer.to_pixels_first_frame.0.bias',\n",
       " 'trained_model.visual_transformer.to_pixels.0.weight',\n",
       " 'trained_model.visual_transformer.to_pixels.0.bias',\n",
       " 'trained_model.to_visual_latent.weight',\n",
       " 'trained_model.to_visual_latent_extra.weight']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in a.keys() if \"to_visual_latent\" in i or \"visual_transformer\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a10cbd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a1 = torch.load('/home/borntowarn/projects/chest-diseases/training/data/CT-RATE/dataset/train_fixed_embeds_not_normalized_base/train_1/train_1_a/train_1_a_2.nii.pt')\n",
    "a2 = torch.load('/home/borntowarn/projects/chest-diseases/backup/dataset/train_fixed_embeds_not_normalized_base/train_1/train_1_a/train_1_a_2.nii.pt')\n",
    "\n",
    "torch.allclose(a1, a2, atol=1e-6, rtol=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cd3c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество файлов для сравнения: 80\n",
      "Файл valid_147_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_147_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_153_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_153_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_163_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_163_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_163_b_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_163_b_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_163_c_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_163_c_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_163_d_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_163_d_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_166_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_166_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_16_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_16_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_177_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_177_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_182_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_182_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_185_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_191_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_191_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_191_b_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_191_b_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_214_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_214_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_245_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_245_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_251_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_251_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_259_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_259_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_260_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_260_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_261_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_261_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_283_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_299_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_299_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_302_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_302_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_314_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_314_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_318_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_318_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_329_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_329_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_337_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_337_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_338_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_338_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_340_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_340_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_340_a_3.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_340_a_4.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_340_a_5.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_353_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_353_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_394_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_394_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_412_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_412_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_416_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_416_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_416_b_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_416_b_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_417_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_417_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_417_a_3.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_417_a_4.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_417_a_5.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_56_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_56_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_73_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_73_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_8_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_8_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_91_a_1.pt: тензоры НЕ совпадают (allclose=False)\n",
      "Файл valid_91_a_2.pt: тензоры НЕ совпадают (allclose=False)\n",
      "\n",
      "Совпадающих тензоров (allclose=True): 0\n",
      "Не совпадающих тензоров (allclose=False): 80\n",
      "Файлы, которых нет в vit: {'valid_361_a_1.pt', 'valid_218_a_1.pt', 'valid_410_c_1.pt', 'valid_150_a_2.pt', 'valid_332_a_2.pt', 'valid_105_a_2.pt', 'valid_387_a_2.pt', 'valid_175_a_2.pt', 'valid_148_a_1.pt', 'valid_47_a_2.pt', 'valid_197_a_1.pt', 'valid_327_a_1.pt', 'valid_111_a_2.pt', 'valid_141_a_2.pt', 'valid_84_a_1.pt', 'valid_275_a_2.pt', 'valid_319_a_1.pt', 'valid_240_a_1.pt', 'valid_176_a_1.pt', 'valid_254_a_2.pt', 'valid_89_a_2.pt', 'valid_310_a_1.pt', 'valid_186_a_1.pt', 'valid_48_a_1.pt', 'valid_349_a_2.pt', 'valid_13_a_2.pt', 'valid_231_a_1.pt', 'valid_247_a_2.pt', 'valid_71_b_2.pt', 'valid_331_b_2.pt', 'valid_400_a_1.pt', 'valid_90_a_1.pt', 'valid_175_a_1.pt', 'valid_41_b_2.pt', 'valid_118_d_2.pt', 'valid_28_a_1.pt', 'valid_277_c_1.pt', 'valid_349_a_1.pt', 'valid_354_a_2.pt', 'valid_101_a_1.pt', 'valid_323_a_2.pt', 'valid_2_a_2.pt', 'valid_23_a_1.pt', 'valid_61_a_1.pt', 'valid_9_a_2.pt', 'valid_320_a_1.pt', 'valid_250_a_2.pt', 'valid_350_a_1.pt', 'valid_196_a_1.pt', 'valid_6_a_1.pt', 'valid_105_a_1.pt', 'valid_102_a_1.pt', 'valid_62_b_1.pt', 'valid_370_a_2.pt', 'valid_60_c_2.pt', 'valid_12_a_1.pt', 'valid_363_a_2.pt', 'valid_104_a_2.pt', 'valid_303_a_2.pt', 'valid_90_c_1.pt', 'valid_298_a_1.pt', 'valid_262_a_1.pt', 'valid_304_a_1.pt', 'valid_206_a_2.pt', 'valid_215_a_1.pt', 'valid_250_c_1.pt', 'valid_258_d_1.pt', 'valid_414_b_2.pt', 'valid_285_a_2.pt', 'valid_375_a_1.pt', 'valid_398_a_1.pt', 'valid_146_a_2.pt', 'valid_15_a_2.pt', 'valid_333_a_1.pt', 'valid_365_a_4.pt', 'valid_312_a_1.pt', 'valid_150_a_1.pt', 'valid_280_a_1.pt', 'valid_364_a_2.pt', 'valid_250_b_1.pt', 'valid_162_a_1.pt', 'valid_311_a_1.pt', 'valid_369_a_2.pt', 'valid_277_c_2.pt', 'valid_89_a_1.pt', 'valid_128_a_2.pt', 'valid_151_a_2.pt', 'valid_169_a_2.pt', 'valid_277_a_2.pt', 'valid_307_a_1.pt', 'valid_124_a_1.pt', 'valid_386_a_2.pt', 'valid_234_a_2.pt', 'valid_29_a_2.pt', 'valid_255_a_2.pt', 'valid_223_a_2.pt', 'valid_90_a_2.pt', 'valid_188_a_2.pt', 'valid_168_b_2.pt', 'valid_199_a_2.pt', 'valid_53_b_2.pt', 'valid_309_a_1.pt', 'valid_233_a_1.pt', 'valid_118_d_1.pt', 'valid_139_a_2.pt', 'valid_20_a_2.pt', 'valid_327_a_2.pt', 'valid_71_b_1.pt', 'valid_226_a_1.pt', 'valid_325_a_1.pt', 'valid_87_a_2.pt', 'valid_272_a_1.pt', 'valid_231_a_2.pt', 'valid_332_b_1.pt', 'valid_210_a_2.pt', 'valid_296_a_1.pt', 'valid_34_a_2.pt', 'valid_277_a_1.pt', 'valid_209_a_1.pt', 'valid_132_a_2.pt', 'valid_55_a_2.pt', 'valid_379_a_2.pt', 'valid_27_b_1.pt', 'valid_79_a_1.pt', 'valid_343_a_2.pt', 'valid_133_a_1.pt', 'valid_347_a_2.pt', 'valid_243_a_1.pt', 'valid_380_a_1.pt', 'valid_365_a_3.pt', 'valid_308_a_2.pt', 'valid_167_a_2.pt', 'valid_126_a_2.pt', 'valid_326_a_2.pt', 'valid_250_c_2.pt', 'valid_189_a_2.pt', 'valid_99_a_2.pt', 'valid_174_a_1.pt', 'valid_205_a_1.pt', 'valid_134_a_2.pt', 'valid_36_a_1.pt', 'valid_300_a_2.pt', 'valid_363_a_1.pt', 'valid_27_b_2.pt', 'valid_211_a_1.pt', 'valid_335_a_1.pt', 'valid_346_a_2.pt', 'valid_246_a_1.pt', 'valid_351_a_2.pt', 'valid_58_a_1.pt', 'valid_81_a_2.pt', 'valid_344_a_2.pt', 'valid_184_a_2.pt', 'valid_43_a_1.pt', 'valid_111_a_1.pt', 'valid_371_a_1.pt', 'valid_69_a_2.pt', 'valid_410_a_2.pt', 'valid_92_a_2.pt', 'valid_357_b_2.pt', 'valid_383_a_1.pt', 'valid_107_b_1.pt', 'valid_222_a_2.pt', 'valid_288_a_2.pt', 'valid_22_a_2.pt', 'valid_389_a_2.pt', 'valid_115_a_2.pt', 'valid_202_a_2.pt', 'valid_101_b_1.pt', 'valid_335_a_2.pt', 'valid_64_a_3.pt', 'valid_254_a_1.pt', 'valid_351_a_1.pt', 'valid_404_a_1.pt', 'valid_188_a_1.pt', 'valid_30_a_1.pt', 'valid_211_a_2.pt', 'valid_117_b_2.pt', 'valid_277_g_2.pt', 'valid_258_b_1.pt', 'valid_173_a_1.pt', 'valid_188_b_1.pt', 'valid_347_a_1.pt', 'valid_373_b_1.pt', 'valid_209_a_2.pt', 'valid_103_a_1.pt', 'valid_138_a_1.pt', 'valid_307_a_2.pt', 'valid_35_a_1.pt', 'valid_389_b_2.pt', 'valid_201_a_2.pt', 'valid_144_a_1.pt', 'valid_95_a_2.pt', 'valid_300_a_1.pt', 'valid_414_b_1.pt', 'valid_108_a_2.pt', 'valid_369_a_1.pt', 'valid_306_a_1.pt', 'valid_341_a_1.pt', 'valid_80_a_2.pt', 'valid_72_a_2.pt', 'valid_424_a_2.pt', 'valid_121_a_2.pt', 'valid_107_b_2.pt', 'valid_250_f_1.pt', 'valid_164_a_2.pt', 'valid_118_b_2.pt', 'valid_196_a_2.pt', 'valid_140_a_1.pt', 'valid_219_a_1.pt', 'valid_205_a_2.pt', 'valid_94_a_2.pt', 'valid_262_a_2.pt', 'valid_342_a_2.pt', 'valid_388_b_2.pt', 'valid_133_a_2.pt', 'valid_46_a_1.pt', 'valid_7_b_1.pt', 'valid_68_a_1.pt', 'valid_216_a_2.pt', 'valid_390_a_1.pt', 'valid_97_a_2.pt', 'valid_85_a_2.pt', 'valid_187_a_2.pt', 'valid_149_a_2.pt', 'valid_407_a_1.pt', 'valid_333_a_2.pt', 'valid_381_a_1.pt', 'valid_223_a_1.pt', 'valid_114_a_1.pt', 'valid_258_d_2.pt', 'valid_220_a_1.pt', 'valid_206_a_1.pt', 'valid_411_a_1.pt', 'valid_279_a_1.pt', 'valid_244_a_2.pt', 'valid_129_a_1.pt', 'valid_100_a_1.pt', 'valid_112_a_1.pt', 'valid_317_a_1.pt', 'valid_331_d_1.pt', 'valid_88_a_1.pt', 'valid_47_a_1.pt', 'valid_379_a_1.pt', 'valid_240_a_2.pt', 'valid_172_a_2.pt', 'valid_90_b_1.pt', 'valid_173_a_2.pt', 'valid_17_a_2.pt', 'valid_50_a_1.pt', 'valid_264_a_2.pt', 'valid_114_b_2.pt', 'valid_66_a_1.pt', 'valid_158_a_1.pt', 'valid_94_a_1.pt', 'valid_171_a_1.pt', 'valid_41_b_1.pt', 'valid_424_a_1.pt', 'valid_248_a_2.pt', 'valid_162_a_2.pt', 'valid_248_a_1.pt', 'valid_35_a_2.pt', 'valid_250_b_2.pt', 'valid_269_a_1.pt', 'valid_168_a_2.pt', 'valid_336_a_1.pt', 'valid_27_a_2.pt', 'valid_286_a_2.pt', 'valid_118_a_2.pt', 'valid_146_a_1.pt', 'valid_10_a_2.pt', 'valid_58_a_2.pt', 'valid_188_b_2.pt', 'valid_110_a_1.pt', 'valid_10_a_1.pt', 'valid_215_b_1.pt', 'valid_212_b_1.pt', 'valid_319_a_2.pt', 'valid_221_a_1.pt', 'valid_193_a_2.pt', 'valid_264_a_1.pt', 'valid_294_a_1.pt', 'valid_207_a_1.pt', 'valid_118_c_1.pt', 'valid_59_a_1.pt', 'valid_71_a_1.pt', 'valid_62_a_1.pt', 'valid_4_a_1.pt', 'valid_357_d_2.pt', 'valid_65_a_1.pt', 'valid_130_a_1.pt', 'valid_320_a_2.pt', 'valid_419_a_2.pt', 'valid_332_a_1.pt', 'valid_321_a_1.pt', 'valid_324_a_2.pt', 'valid_410_c_2.pt', 'valid_157_a_2.pt', 'valid_330_a_1.pt', 'valid_40_a_2.pt', 'valid_200_a_1.pt', 'valid_41_a_1.pt', 'valid_225_a_1.pt', 'valid_72_a_1.pt', 'valid_278_a_2.pt', 'valid_26_a_2.pt', 'valid_357_c_2.pt', 'valid_304_a_2.pt', 'valid_208_a_4.pt', 'valid_194_b_2.pt', 'valid_180_b_1.pt', 'valid_402_a_1.pt', 'valid_376_a_1.pt', 'valid_387_b_1.pt', 'valid_310_a_2.pt', 'valid_420_a_1.pt', 'valid_63_a_2.pt', 'valid_263_a_1.pt', 'valid_208_a_5.pt', 'valid_331_e_2.pt', 'valid_136_a_1.pt', 'valid_410_a_1.pt', 'valid_40_a_1.pt', 'valid_244_a_1.pt', 'valid_289_a_1.pt', 'valid_239_a_2.pt', 'valid_39_a_2.pt', 'valid_107_a_1.pt', 'valid_237_a_1.pt', 'valid_83_a_2.pt', 'valid_284_a_1.pt', 'valid_6_a_2.pt', 'valid_63_b_2.pt', 'valid_131_a_1.pt', 'valid_31_a_1.pt', 'valid_203_a_2.pt', 'valid_250_d_2.pt', 'valid_15_a_1.pt', 'valid_357_a_2.pt', 'valid_290_a_2.pt', 'valid_298_a_2.pt', 'valid_118_c_2.pt', 'valid_388_b_1.pt', 'valid_156_a_2.pt', 'valid_4_b_2.pt', 'valid_61_a_2.pt', 'valid_79_a_2.pt', 'valid_131_a_2.pt', 'valid_11_a_1.pt', 'valid_418_a_1.pt', 'valid_148_a_2.pt', 'valid_18_a_2.pt', 'valid_279_a_2.pt', 'valid_208_a_3.pt', 'valid_60_b_1.pt', 'valid_120_a_1.pt', 'valid_303_a_1.pt', 'valid_93_a_2.pt', 'valid_9_a_1.pt', 'valid_112_a_2.pt', 'valid_113_a_2.pt', 'valid_250_f_2.pt', 'valid_295_a_2.pt', 'valid_323_a_1.pt', 'valid_4_b_1.pt', 'valid_267_a_1.pt', 'valid_11_a_2.pt', 'valid_31_a_2.pt', 'valid_409_a_1.pt', 'valid_95_a_1.pt', 'valid_201_a_1.pt', 'valid_82_a_2.pt', 'valid_362_a_1.pt', 'valid_382_b_2.pt', 'valid_21_a_1.pt', 'valid_145_a_2.pt', 'valid_71_a_2.pt', 'valid_130_a_2.pt', 'valid_388_a_2.pt', 'valid_408_a_1.pt', 'valid_392_a_2.pt', 'valid_53_b_1.pt', 'valid_229_a_2.pt', 'valid_328_a_2.pt', 'valid_322_a_2.pt', 'valid_352_a_2.pt', 'valid_277_b_2.pt', 'valid_186_a_2.pt', 'valid_418_a_2.pt', 'valid_101_b_2.pt', 'valid_27_a_1.pt', 'valid_159_a_2.pt', 'valid_159_a_1.pt', 'valid_25_a_1.pt', 'valid_155_a_2.pt', 'valid_413_a_1.pt', 'valid_237_a_2.pt', 'valid_362_b_2.pt', 'valid_162_b_1.pt', 'valid_321_a_2.pt', 'valid_200_a_2.pt', 'valid_213_a_2.pt', 'valid_168_a_1.pt', 'valid_381_a_2.pt', 'valid_331_e_1.pt', 'valid_257_a_3.pt', 'valid_364_a_1.pt', 'valid_399_a_1.pt', 'valid_274_c_2.pt', 'valid_104_a_1.pt', 'valid_222_a_1.pt', 'valid_171_a_2.pt', 'valid_57_a_2.pt', 'valid_109_a_3.pt', 'valid_354_a_1.pt', 'valid_389_a_1.pt', 'valid_155_a_1.pt', 'valid_263_a_2.pt', 'valid_365_a_1.pt', 'valid_282_a_2.pt', 'valid_98_a_2.pt', 'valid_239_a_1.pt', 'valid_419_a_1.pt', 'valid_193_a_1.pt', 'valid_135_a_2.pt', 'valid_118_a_1.pt', 'valid_70_a_2.pt', 'valid_41_a_2.pt', 'valid_70_a_1.pt', 'valid_309_a_2.pt', 'valid_98_a_1.pt', 'valid_108_a_1.pt', 'valid_384_a_1.pt', 'valid_17_a_1.pt', 'valid_358_a_1.pt', 'valid_117_a_1.pt', 'valid_376_a_2.pt', 'valid_372_a_1.pt', 'valid_38_a_2.pt', 'valid_81_a_1.pt', 'valid_183_a_1.pt', 'valid_308_a_1.pt', 'valid_161_a_1.pt', 'valid_128_a_1.pt', 'valid_423_a_2.pt', 'valid_185_a_1.pt', 'valid_226_a_2.pt', 'valid_20_b_2.pt', 'valid_103_a_2.pt', 'valid_138_a_2.pt', 'valid_122_a_1.pt', 'valid_297_a_1.pt', 'valid_287_a_2.pt', 'valid_102_a_2.pt', 'valid_202_a_1.pt', 'valid_387_a_1.pt', 'valid_121_a_1.pt', 'valid_341_a_2.pt', 'valid_385_a_1.pt', 'valid_218_a_2.pt', 'valid_422_a_1.pt', 'valid_413_a_2.pt', 'valid_423_b_1.pt', 'valid_215_c_1.pt', 'valid_351_b_2.pt', 'valid_194_a_1.pt', 'valid_270_a_2.pt', 'valid_230_a_2.pt', 'valid_409_a_2.pt', 'valid_62_a_2.pt', 'valid_3_a_2.pt', 'valid_362_a_2.pt', 'valid_332_b_2.pt', 'valid_54_a_1.pt', 'valid_406_a_2.pt', 'valid_208_a_1.pt', 'valid_282_a_1.pt', 'valid_339_a_2.pt', 'valid_276_a_2.pt', 'valid_34_a_1.pt', 'valid_271_a_1.pt', 'valid_13_a_1.pt', 'valid_204_a_1.pt', 'valid_202_b_2.pt', 'valid_356_a_1.pt', 'valid_33_a_2.pt', 'valid_135_a_1.pt', 'valid_351_b_1.pt', 'valid_207_a_2.pt', 'valid_366_a_1.pt', 'valid_275_a_1.pt', 'valid_151_b_1.pt', 'valid_183_a_2.pt', 'valid_229_a_1.pt', 'valid_137_a_1.pt', 'valid_421_a_2.pt', 'valid_232_a_2.pt', 'valid_333_b_1.pt', 'valid_333_b_2.pt', 'valid_368_a_2.pt', 'valid_152_a_1.pt', 'valid_113_a_1.pt', 'valid_7_a_1.pt', 'valid_306_a_2.pt', 'valid_382_c_2.pt', 'valid_281_a_2.pt', 'valid_192_a_1.pt', 'valid_357_d_1.pt', 'valid_257_a_4.pt', 'valid_357_b_1.pt', 'valid_22_a_1.pt', 'valid_139_a_1.pt', 'valid_253_a_1.pt', 'valid_415_a_1.pt', 'valid_274_a_1.pt', 'valid_305_a_1.pt', 'valid_315_a_2.pt', 'valid_255_a_1.pt', 'valid_238_a_1.pt', 'valid_176_a_2.pt', 'valid_96_a_1.pt', 'valid_109_a_2.pt', 'valid_195_a_2.pt', 'valid_281_a_1.pt', 'valid_250_a_1.pt', 'valid_293_a_2.pt', 'valid_24_a_2.pt', 'valid_355_a_2.pt', 'valid_225_b_2.pt', 'valid_48_a_2.pt', 'valid_399_a_2.pt', 'valid_3_a_1.pt', 'valid_165_a_1.pt', 'valid_342_a_1.pt', 'valid_84_a_2.pt', 'valid_212_a_1.pt', 'valid_208_a_2.pt', 'valid_80_a_1.pt', 'valid_414_a_2.pt', 'valid_50_a_2.pt', 'valid_20_a_1.pt', 'valid_161_a_2.pt', 'valid_250_e_2.pt', 'valid_124_a_2.pt', 'valid_266_a_1.pt', 'valid_141_a_1.pt', 'valid_277_f_2.pt', 'valid_370_a_1.pt', 'valid_45_a_1.pt', 'valid_268_a_1.pt', 'valid_64_c_2.pt', 'valid_242_a_1.pt', 'valid_257_a_2.pt', 'valid_194_b_1.pt', 'valid_32_a_1.pt', 'valid_391_a_2.pt', 'valid_136_a_2.pt', 'valid_19_a_1.pt', 'valid_265_a_1.pt', 'valid_289_a_2.pt', 'valid_392_a_1.pt', 'valid_117_b_1.pt', 'valid_236_a_1.pt', 'valid_100_a_2.pt', 'valid_116_a_1.pt', 'valid_373_b_2.pt', 'valid_266_a_2.pt', 'valid_7_b_2.pt', 'valid_44_a_2.pt', 'valid_123_a_1.pt', 'valid_170_b_2.pt', 'valid_178_a_2.pt', 'valid_219_a_2.pt', 'valid_75_a_1.pt', 'valid_293_a_1.pt', 'valid_190_a_1.pt', 'valid_258_a_1.pt', 'valid_415_a_2.pt', 'valid_227_a_2.pt', 'valid_277_d_1.pt', 'valid_77_a_2.pt', 'valid_217_a_2.pt', 'valid_125_a_1.pt', 'valid_406_a_1.pt', 'valid_82_a_1.pt', 'valid_24_a_1.pt', 'valid_59_a_2.pt', 'valid_301_a_1.pt', 'valid_38_a_1.pt', 'valid_53_a_2.pt', 'valid_236_a_2.pt', 'valid_140_a_2.pt', 'valid_355_a_1.pt', 'valid_249_a_1.pt', 'valid_393_a_1.pt', 'valid_288_a_1.pt', 'valid_317_a_2.pt', 'valid_256_a_2.pt', 'valid_190_a_2.pt', 'valid_397_a_1.pt', 'valid_373_a_2.pt', 'valid_390_a_2.pt', 'valid_356_a_2.pt', 'valid_106_a_1.pt', 'valid_274_b_2.pt', 'valid_287_a_1.pt', 'valid_83_a_1.pt', 'valid_331_a_1.pt', 'valid_258_a_2.pt', 'valid_242_a_2.pt', 'valid_45_a_2.pt', 'valid_346_a_1.pt', 'valid_39_a_1.pt', 'valid_315_a_1.pt', 'valid_60_a_2.pt', 'valid_54_a_2.pt', 'valid_180_a_2.pt', 'valid_331_b_1.pt', 'valid_247_a_1.pt', 'valid_137_a_2.pt', 'valid_331_c_2.pt', 'valid_250_d_1.pt', 'valid_203_a_1.pt', 'valid_151_c_1.pt', 'valid_174_a_2.pt', 'valid_227_a_1.pt', 'valid_213_a_1.pt', 'valid_401_a_2.pt', 'valid_26_a_1.pt', 'valid_216_a_1.pt', 'valid_421_a_1.pt', 'valid_366_a_2.pt', 'valid_277_g_1.pt', 'valid_60_a_1.pt', 'valid_170_a_2.pt', 'valid_210_a_1.pt', 'valid_257_a_1.pt', 'valid_160_a_2.pt', 'valid_62_c_2.pt', 'valid_202_b_1.pt', 'valid_277_d_2.pt', 'valid_165_a_2.pt', 'valid_367_a_1.pt', 'valid_355_b_1.pt', 'valid_198_a_1.pt', 'valid_181_a_1.pt', 'valid_18_a_1.pt', 'valid_410_b_2.pt', 'valid_403_a_1.pt', 'valid_225_b_1.pt', 'valid_277_b_1.pt', 'valid_44_a_1.pt', 'valid_62_b_2.pt', 'valid_290_a_1.pt', 'valid_348_a_1.pt', 'valid_123_a_2.pt', 'valid_64_b_2.pt', 'valid_378_a_2.pt', 'valid_225_a_2.pt', 'valid_194_a_2.pt', 'valid_360_a_1.pt', 'valid_294_a_2.pt', 'valid_359_a_1.pt', 'valid_1_a_1.pt', 'valid_90_b_2.pt', 'valid_212_b_2.pt', 'valid_52_a_1.pt', 'valid_395_a_1.pt', 'valid_143_a_2.pt', 'valid_64_a_2.pt', 'valid_99_a_1.pt', 'valid_170_b_1.pt', 'valid_162_b_2.pt', 'valid_374_a_2.pt', 'valid_109_a_1.pt', 'valid_107_a_2.pt', 'valid_267_a_2.pt', 'valid_224_a_1.pt', 'valid_362_b_1.pt', 'valid_37_a_1.pt', 'valid_78_a_1.pt', 'valid_106_a_4.pt', 'valid_404_a_2.pt', 'valid_348_a_2.pt', 'valid_74_a_1.pt', 'valid_357_e_2.pt', 'valid_382_b_1.pt', 'valid_350_a_2.pt', 'valid_268_a_2.pt', 'valid_368_a_1.pt', 'valid_235_a_1.pt', 'valid_12_a_2.pt', 'valid_179_a_2.pt', 'valid_199_a_1.pt', 'valid_420_a_2.pt', 'valid_76_a_1.pt', 'valid_357_e_1.pt', 'valid_215_b_2.pt', 'valid_180_a_1.pt', 'valid_51_a_2.pt', 'valid_345_a_1.pt', 'valid_359_a_2.pt', 'valid_252_a_1.pt', 'valid_423_c_1.pt', 'valid_343_a_1.pt', 'valid_374_a_1.pt', 'valid_198_a_2.pt', 'valid_156_a_1.pt', 'valid_119_a_2.pt', 'valid_382_a_1.pt', 'valid_263_b_2.pt', 'valid_122_a_2.pt', 'valid_273_a_1.pt', 'valid_397_a_2.pt', 'valid_106_a_2.pt', 'valid_410_b_1.pt', 'valid_334_a_3.pt', 'valid_157_a_1.pt', 'valid_184_a_1.pt', 'valid_49_a_1.pt', 'valid_1_a_2.pt', 'valid_285_a_1.pt', 'valid_271_a_2.pt', 'valid_154_a_2.pt', 'valid_322_a_1.pt', 'valid_331_a_2.pt', 'valid_212_a_2.pt', 'valid_151_a_1.pt', 'valid_75_a_2.pt', 'valid_220_a_2.pt', 'valid_160_a_1.pt', 'valid_234_a_1.pt', 'valid_97_a_1.pt', 'valid_241_a_2.pt', 'valid_74_a_2.pt', 'valid_69_a_1.pt', 'valid_87_a_1.pt', 'valid_85_a_1.pt', 'valid_391_a_1.pt', 'valid_33_a_1.pt', 'valid_280_a_2.pt', 'valid_52_a_2.pt', 'valid_86_a_1.pt', 'valid_143_a_1.pt', 'valid_263_b_1.pt', 'valid_134_a_1.pt', 'valid_396_a_1.pt', 'valid_169_a_1.pt', 'valid_62_c_1.pt', 'valid_277_f_1.pt', 'valid_382_a_2.pt', 'valid_334_a_1.pt', 'valid_339_a_1.pt', 'valid_7_a_2.pt', 'valid_278_b_1.pt', 'valid_60_b_2.pt', 'valid_93_a_1.pt', 'valid_405_a_1.pt', 'valid_414_a_1.pt', 'valid_43_a_2.pt', 'valid_77_a_1.pt', 'valid_393_a_2.pt', 'valid_331_d_2.pt', 'valid_265_a_2.pt', 'valid_114_b_1.pt', 'valid_92_a_1.pt', 'valid_387_b_2.pt', 'valid_331_c_1.pt', 'valid_23_a_2.pt', 'valid_357_c_1.pt', 'valid_249_a_2.pt', 'valid_119_a_1.pt', 'valid_252_a_2.pt', 'valid_324_a_1.pt', 'valid_154_a_1.pt', 'valid_57_a_1.pt', 'valid_132_a_1.pt', 'valid_326_a_1.pt', 'valid_286_a_1.pt', 'valid_67_a_1.pt', 'valid_241_a_1.pt', 'valid_305_a_2.pt', 'valid_228_a_1.pt', 'valid_229_b_2.pt', 'valid_217_a_1.pt', 'valid_312_a_2.pt', 'valid_55_a_1.pt', 'valid_274_c_1.pt', 'valid_42_a_1.pt', 'valid_29_a_1.pt', 'valid_109_a_4.pt', 'valid_25_a_2.pt', 'valid_210_b_1.pt', 'valid_180_b_2.pt', 'valid_386_a_1.pt', 'valid_407_a_2.pt', 'valid_229_b_1.pt', 'valid_389_b_1.pt', 'valid_172_a_1.pt', 'valid_96_b_2.pt', 'valid_423_b_2.pt', 'valid_215_c_2.pt', 'valid_278_b_2.pt', 'valid_221_a_2.pt', 'valid_371_a_2.pt', 'valid_151_b_2.pt', 'valid_301_a_2.pt', 'valid_5_a_1.pt', 'valid_28_a_2.pt', 'valid_345_a_2.pt', 'valid_64_b_1.pt', 'valid_246_a_2.pt', 'valid_120_a_2.pt', 'valid_328_a_1.pt', 'valid_378_a_1.pt', 'valid_334_a_2.pt', 'valid_106_a_3.pt', 'valid_355_b_2.pt', 'valid_167_a_1.pt', 'valid_277_e_2.pt', 'valid_178_a_1.pt', 'valid_330_a_2.pt', 'valid_125_a_2.pt', 'valid_256_a_1.pt', 'valid_181_a_2.pt', 'valid_292_a_1.pt', 'valid_115_a_1.pt', 'valid_425_a_2.pt', 'valid_313_a_1.pt', 'valid_145_a_1.pt', 'valid_408_a_2.pt', 'valid_382_c_1.pt', 'valid_114_a_2.pt', 'valid_197_a_2.pt', 'valid_357_a_1.pt', 'valid_344_a_1.pt', 'valid_291_a_1.pt', 'valid_313_a_2.pt', 'valid_400_a_2.pt', 'valid_352_a_1.pt', 'valid_117_a_2.pt', 'valid_422_a_2.pt', 'valid_168_b_1.pt', 'valid_336_a_2.pt', 'valid_78_a_2.pt', 'valid_373_a_1.pt', 'valid_96_a_2.pt', 'valid_118_b_1.pt', 'valid_151_c_2.pt', 'valid_316_a_2.pt', 'valid_423_c_2.pt', 'valid_53_a_1.pt', 'valid_278_a_1.pt', 'valid_380_a_2.pt', 'valid_403_b_1.pt', 'valid_116_a_2.pt', 'valid_63_a_1.pt', 'valid_90_c_2.pt', 'valid_192_a_2.pt', 'valid_232_a_1.pt', 'valid_14_a_1.pt', 'valid_258_c_1.pt', 'valid_250_e_1.pt', 'valid_21_a_2.pt', 'valid_86_a_2.pt', 'valid_425_a_1.pt', 'valid_360_a_2.pt', 'valid_88_a_2.pt', 'valid_377_a_2.pt', 'valid_325_a_2.pt', 'valid_60_c_1.pt', 'valid_284_a_2.pt', 'valid_257_a_5.pt', 'valid_14_a_2.pt', 'valid_291_a_2.pt', 'valid_377_a_1.pt', 'valid_5_a_2.pt', 'valid_423_a_1.pt', 'valid_20_b_1.pt', 'valid_401_a_1.pt', 'valid_388_a_1.pt', 'valid_361_a_2.pt', 'valid_2_a_1.pt', 'valid_46_a_2.pt', 'valid_127_a_2.pt', 'valid_126_a_1.pt', 'valid_170_a_1.pt', 'valid_316_a_1.pt', 'valid_277_e_1.pt', 'valid_64_c_1.pt', 'valid_158_a_2.pt', 'valid_195_a_1.pt', 'valid_149_a_1.pt', 'valid_215_a_2.pt', 'valid_403_b_2.pt', 'valid_365_a_2.pt', 'valid_187_a_1.pt', 'valid_367_a_2.pt', 'valid_179_a_1.pt', 'valid_51_a_1.pt', 'valid_109_a_5.pt', 'valid_204_a_2.pt', 'valid_189_a_1.pt', 'valid_96_b_1.pt', 'valid_365_a_5.pt', 'valid_63_b_1.pt', 'valid_270_a_1.pt', 'valid_142_a_1.pt', 'valid_224_a_2.pt', 'valid_127_a_1.pt', 'valid_64_a_1.pt', 'valid_295_a_1.pt', 'valid_230_a_1.pt', 'valid_49_a_2.pt', 'valid_276_a_1.pt', 'valid_274_b_1.pt', 'valid_243_a_2.pt', 'valid_164_a_1.pt', 'valid_4_a_2.pt', 'valid_258_b_2.pt'}\n",
      "\n",
      "Список файлов с несовпадающими тензорами:\n",
      "valid_147_a_1.pt\n",
      "valid_147_a_2.pt\n",
      "valid_153_a_1.pt\n",
      "valid_153_a_2.pt\n",
      "valid_163_a_1.pt\n",
      "valid_163_a_2.pt\n",
      "valid_163_b_1.pt\n",
      "valid_163_b_2.pt\n",
      "valid_163_c_1.pt\n",
      "valid_163_c_2.pt\n",
      "valid_163_d_1.pt\n",
      "valid_163_d_2.pt\n",
      "valid_166_a_1.pt\n",
      "valid_166_a_2.pt\n",
      "valid_16_a_1.pt\n",
      "valid_16_a_2.pt\n",
      "valid_177_a_1.pt\n",
      "valid_177_a_2.pt\n",
      "valid_182_a_1.pt\n",
      "valid_182_a_2.pt\n",
      "valid_185_a_2.pt\n",
      "valid_191_a_1.pt\n",
      "valid_191_a_2.pt\n",
      "valid_191_b_1.pt\n",
      "valid_191_b_2.pt\n",
      "valid_214_a_1.pt\n",
      "valid_214_a_2.pt\n",
      "valid_245_a_1.pt\n",
      "valid_245_a_2.pt\n",
      "valid_251_a_1.pt\n",
      "valid_251_a_2.pt\n",
      "valid_259_a_1.pt\n",
      "valid_259_a_2.pt\n",
      "valid_260_a_1.pt\n",
      "valid_260_a_2.pt\n",
      "valid_261_a_1.pt\n",
      "valid_261_a_2.pt\n",
      "valid_283_a_1.pt\n",
      "valid_299_a_1.pt\n",
      "valid_299_a_2.pt\n",
      "valid_302_a_1.pt\n",
      "valid_302_a_2.pt\n",
      "valid_314_a_1.pt\n",
      "valid_314_a_2.pt\n",
      "valid_318_a_1.pt\n",
      "valid_318_a_2.pt\n",
      "valid_329_a_1.pt\n",
      "valid_329_a_2.pt\n",
      "valid_337_a_1.pt\n",
      "valid_337_a_2.pt\n",
      "valid_338_a_1.pt\n",
      "valid_338_a_2.pt\n",
      "valid_340_a_1.pt\n",
      "valid_340_a_2.pt\n",
      "valid_340_a_3.pt\n",
      "valid_340_a_4.pt\n",
      "valid_340_a_5.pt\n",
      "valid_353_a_1.pt\n",
      "valid_353_a_2.pt\n",
      "valid_394_a_1.pt\n",
      "valid_394_a_2.pt\n",
      "valid_412_a_1.pt\n",
      "valid_412_a_2.pt\n",
      "valid_416_a_1.pt\n",
      "valid_416_a_2.pt\n",
      "valid_416_b_1.pt\n",
      "valid_416_b_2.pt\n",
      "valid_417_a_1.pt\n",
      "valid_417_a_2.pt\n",
      "valid_417_a_3.pt\n",
      "valid_417_a_4.pt\n",
      "valid_417_a_5.pt\n",
      "valid_56_a_1.pt\n",
      "valid_56_a_2.pt\n",
      "valid_73_a_1.pt\n",
      "valid_73_a_2.pt\n",
      "valid_8_a_1.pt\n",
      "valid_8_a_2.pt\n",
      "valid_91_a_1.pt\n",
      "valid_91_a_2.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Пути к папкам с тензорами\n",
    "base_dir = \"/home/borntowarn/projects/chest-diseases/training/data/CT-RATE/cached_latents\"\n",
    "vit_dir = os.path.join(base_dir, \"vit_0_1\")\n",
    "orig_dir = os.path.join(base_dir, \"vit_final1\")\n",
    "\n",
    "# Получаем список файлов в обеих папках\n",
    "vit_files = set(os.listdir(vit_dir))\n",
    "orig_files = set(os.listdir(orig_dir))\n",
    "\n",
    "# Находим общие файлы\n",
    "common_files = vit_files & orig_files\n",
    "\n",
    "print(f\"Общее количество файлов для сравнения: {len(common_files)}\")\n",
    "\n",
    "not_allclose = []\n",
    "allclose = []\n",
    "missing_in_vit = orig_files - vit_files\n",
    "missing_in_orig = vit_files - orig_files\n",
    "\n",
    "for fname in sorted(common_files):\n",
    "    vit_path = os.path.join(vit_dir, fname)\n",
    "    orig_path = os.path.join(orig_dir, fname)\n",
    "    try:\n",
    "        vit_tensor = torch.load(vit_path, map_location=\"cpu\")\n",
    "        orig_tensor = torch.load(orig_path, map_location=\"cpu\")\n",
    "        # Приводим к одному типу, если нужно\n",
    "        if vit_tensor.dtype != orig_tensor.dtype:\n",
    "            orig_tensor = orig_tensor.to(vit_tensor.dtype)\n",
    "        # Проверяем форму\n",
    "        if vit_tensor.shape != orig_tensor.shape:\n",
    "            print(f\"Файл {fname}: разные формы тензоров: {vit_tensor.shape} vs {orig_tensor.shape}\")\n",
    "            not_allclose.append(fname)\n",
    "            continue\n",
    "        # Проверяем на allclose\n",
    "        if torch.allclose(vit_tensor, orig_tensor, atol=1e-6, rtol=1e-6):\n",
    "            allclose.append(fname)\n",
    "        else:\n",
    "            print(f\"Файл {fname}: тензоры НЕ совпадают (allclose=False)\")\n",
    "            not_allclose.append(fname)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке файла {fname}: {e}\")\n",
    "        not_allclose.append(fname)\n",
    "\n",
    "print(f\"\\nСовпадающих тензоров (allclose=True): {len(allclose)}\")\n",
    "print(f\"Не совпадающих тензоров (allclose=False): {len(not_allclose)}\")\n",
    "if missing_in_vit:\n",
    "    print(f\"Файлы, которых нет в vit: {missing_in_vit}\")\n",
    "if missing_in_orig:\n",
    "    print(f\"Файлы, которых нет в orig: {missing_in_orig}\")\n",
    "\n",
    "if not_allclose:\n",
    "    print(\"\\nСписок файлов с несовпадающими тензорами:\")\n",
    "    for fname in not_allclose:\n",
    "        print(fname)\n",
    "else:\n",
    "    print(\"\\nВсе тензоры совпадают (allclose=True)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6877dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/borntowarn/projects/chest-diseases/training/data/CT-RATE/dataset/metadata/validation_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64ed4384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-32767.0), np.float64(32767.0))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "all_data = []\n",
    "for path in Path('/home/borntowarn/projects/chest-diseases/training/data/CT-RATE/minmax').glob('*.npz'):\n",
    "    data = np.load(path)\n",
    "    all_data.append(data['img_data'])\n",
    "\n",
    "all_data = np.array(all_data)\n",
    "all_data.min(), all_data.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069f71c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RescaleIntercept\n",
       "-1024    2189\n",
       "-8192     850\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RescaleIntercept'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e324b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RescaleSlope\n",
       "1    3039\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RescaleSlope'].value_counts()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
